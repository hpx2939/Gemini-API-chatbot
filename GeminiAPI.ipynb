{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gradio\n",
    "!pip install -q -U google-generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and Setup\n",
    "Gemini API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\.conda\\envs\\yolov7\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set Gemini API sucessfully!!\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "from typing import List, Tuple\n",
    "import gradio as gr\n",
    "import json\n",
    "\n",
    "# Set up Gemini API key\n",
    "## TODO: Fill in your Gemini API in the \"\"\n",
    "GOOGLE_API_KEY=\"\"\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "# Check if you have set your Gemini API successfully\n",
    "# You should see \"Set Gemini API sucessfully!!\" if nothing goes wrong.\n",
    "try:\n",
    "    model.generate_content(\n",
    "      \"test\",\n",
    "    )\n",
    "    print(\"Set Gemini API sucessfully!!\")\n",
    "except:\n",
    "    print(\"There seems to be something wrong with your Gemini API. Please follow our demonstration in the slide to get a correct one.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TODO: Input the prompt in the \"\"\n",
    "prompt_for_summarization = \"\"\n",
    "\n",
    "# function to clear the conversation\n",
    "def reset() -> List:\n",
    "    return []\n",
    "\n",
    "# function to call the model to generate\n",
    "def interact_summarization(prompt: str, article: str, temp = 1.0) -> List[Tuple[str, str]]:\n",
    "    '''\n",
    "      * Arguments\n",
    "\n",
    "        - prompt: the prompt that we use in this section\n",
    "\n",
    "        - article: the article to be summarized\n",
    "\n",
    "        - temp: the temperature parameter of this model. Temperature is used to control the output of the chatbot.\n",
    "                The higher the temperature is, the more creative response you will get.\n",
    "    '''\n",
    "    input = f\"{prompt}\\n{article}\"\n",
    "    response = model.generate_content(\n",
    "      input,\n",
    "      generation_config=genai.types.GenerationConfig(temperature=temp),\n",
    "      safety_settings=[\n",
    "          {\"category\": \"HARM_CATEGORY_HARASSMENT\",\"threshold\": \"BLOCK_NONE\",},\n",
    "          {\"category\": \"HARM_CATEGORY_HATE_SPEECH\",\"threshold\": \"BLOCK_NONE\",},\n",
    "          {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\"threshold\": \"BLOCK_NONE\",},\n",
    "          {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\"threshold\": \"BLOCK_NONE\",},\n",
    "          ]\n",
    "    )\n",
    "\n",
    "    return [(input, response.text)]\n",
    "\n",
    "# function to export the whole conversation log\n",
    "def export_summarization(chatbot: List[Tuple[str, str]], article: str) -> None:\n",
    "    '''\n",
    "    * Arguments\n",
    "\n",
    "      - chatbot: the model itself, the conversation is stored in list of tuples\n",
    "\n",
    "      - article: the article to be summarized\n",
    "\n",
    "    '''\n",
    "    target = {\"chatbot\": chatbot, \"article\": article}\n",
    "    with open(\"part1.json\", \"w\") as file:\n",
    "        json.dump(target, file)\n",
    "\n",
    "\n",
    "# This part constructs the Gradio UI interface\n",
    "with gr.Blocks() as demo:\n",
    "    # gr.Markdown(\"# Part1: Summarization\\nFill in any article you like and let the chatbot summarize it for you!!\")\n",
    "    gr.Markdown(\"# Ask any questions\")\n",
    "    \n",
    "    chatbot = gr.Chatbot()\n",
    "    prompt_textbox = gr.Textbox(label=\"Prompt\", value=prompt_for_summarization, visible=False)\n",
    "    article_textbox = gr.Textbox(label=\"Your questions\", interactive = True)\n",
    "    with gr.Column():\n",
    "        # gr.Markdown(\"#  Temperature\\n Temperature is used to control the output of the chatbot. The higher the temperature is, the more creative response you will get.\")\n",
    "        gr.Markdown(\"#  Diverse answers\")\n",
    "        temperature_slider = gr.Slider(0.0, 1.0, 0.7, step = 0.1)\n",
    "    with gr.Row():\n",
    "        sent_button = gr.Button(value=\"Send\")\n",
    "        reset_button = gr.Button(value=\"Reset\")\n",
    "\n",
    "    with gr.Column():\n",
    "        gr.Markdown(\"#  Save your Result.\\n After you get a satisfied result. Click the export button to recode it.\")\n",
    "        export_button = gr.Button(value=\"Export\")\n",
    "    sent_button.click(interact_summarization, inputs=[prompt_textbox, article_textbox, temperature_slider], outputs=[chatbot])\n",
    "    reset_button.click(reset, outputs=[chatbot])\n",
    "    export_button.click(export_summarization, inputs=[chatbot, article_textbox])\n",
    "\n",
    "\n",
    "demo.launch(debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'panel'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerativeai\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgenai\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpanel\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpn\u001b[39;00m\n\u001b[0;32m      4\u001b[0m genai\u001b[38;5;241m.\u001b[39mconfigure(api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAIzaSyAEXsXrhUU0Re9ptuhtmq6-08epmA5wkvE\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;66;03m#你自己的api_key\u001b[39;00m\n\u001b[0;32m      5\u001b[0m                 transport\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m genai\u001b[38;5;241m.\u001b[39mGenerativeModel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgemini-pro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'panel'"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import panel as pn\n",
    " \n",
    "genai.configure(api_key='',#你自己的api_key\n",
    "                transport='rest')\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "pn.extension()\n",
    " \n",
    "async def callback(contents: str, user: str, instance: pn.chat.ChatInterface):\n",
    "    response = model.generate_content(contents,stream=True)\n",
    "    message = \"\"\n",
    "    for chunk in response:\n",
    "        message += chunk.text\n",
    "        yield message\n",
    " \n",
    "chat_interface = pn.chat.ChatInterface(callback=callback, \n",
    "                                       callback_user=\"Gemini\")\n",
    "chat_interface.send(\n",
    "    \"Gemini\", \n",
    "    user=\"System\", respond=False\n",
    ")\n",
    " \n",
    "chat_interface.servable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
